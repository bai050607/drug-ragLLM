"""
å‘é‡æ•°æ®åº“ç®¡ç†æ¨¡å—

æä¾›å‘é‡æ•°æ®åº“çš„ç»Ÿä¸€ç®¡ç†æ¥å£ï¼Œæ”¯æŒå¤šç§å‘é‡æ•°æ®åº“åç«¯ã€‚
"""

from pathlib import Path
import sys
import os
from typing import List, Optional, Dict, Any, Union
import asyncio

current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root))


from llama_index.core import Document
from llama_index.core.schema import NodeWithScore
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb


class VectorDatabase:
    """å‘é‡æ•°æ®åº“ç®¡ç†ç±»"""
    
    def __init__(self, 
                 embeddings=None,
                 vector_db_path: str = None,
                 collection_name: str = None):
        self.embeddings = embeddings 
        self.vector_db_path = vector_db_path 
        self.collection_name = collection_name 
        self.client = None
        self.vector_store = None
        self.index = None
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self._init_vector_database()
    
    def _init_vector_database(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            # åˆ›å»ºChromaDBå®¢æˆ·ç«¯
            self.client = chromadb.PersistentClient(path=self.vector_db_path)
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            self.collection = self.client.get_or_create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            self.vector_store = ChromaVectorStore(chroma_collection=self.collection)
            
            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            try:
                self.storage_context = StorageContext.from_defaults(
                    persist_dir=self.vector_db_path,
                    vector_store=self.vector_store
                )
            except FileNotFoundError:
                # å¦‚æœæŒä¹…åŒ–ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„å­˜å‚¨ä¸Šä¸‹æ–‡
                self.storage_context = StorageContext.from_defaults(
                    vector_store=self.vector_store
                )
                self.storage_context.persist(persist_dir=self.vector_db_path)
            
            # åˆ›å»ºå‘é‡ç´¢å¼•
            self.index = VectorStoreIndex.from_vector_store(
                vector_store=self.vector_store,
                storage_context=self.storage_context,
                embed_model=self.embeddings
            )
            
            print(f"âœ… æˆåŠŸè¿æ¥åˆ°å‘é‡æ•°æ®åº“: {self.vector_db_path}")
            print(f"ğŸ“Š é›†åˆ '{self.collection_name}' åŒ…å« {self.collection.count()} ä¸ªæ¡ç›®")
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            self.client = None
            self.vector_store = None
            self.index = None
    
    def add_documents(self, documents: List[Document]) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        if not self.index:
            print("âŒ å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return False
        
        try:
            # å°†æ–‡æ¡£æ·»åŠ åˆ°ç´¢å¼•
            for doc in documents:
                self.index.insert(doc)
            
            # æŒä¹…åŒ–å­˜å‚¨
            self.storage_context.persist(persist_dir=self.vector_db_path)
            
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
            return True
            
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def delete_documents(self, doc_ids: List[str]) -> bool:
        """ä»å‘é‡æ•°æ®åº“åˆ é™¤æ–‡æ¡£"""
        if not self.index:
            print("âŒ å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return False
        
        try:
            # åˆ é™¤æ–‡æ¡£
            for doc_id in doc_ids:
                self.index.delete_ref_doc(doc_id)
            
            # æŒä¹…åŒ–å­˜å‚¨
            self.storage_context.persist(persist_dir=self.vector_db_path)
            
            print(f"âœ… æˆåŠŸåˆ é™¤ {len(doc_ids)} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def search(self, query: str, top_k: int = None) -> List[NodeWithScore]:
        """åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢"""
        if not self.index:
            print("âŒ å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return []
        
        top_k = top_k 
        
        try:
            # åˆ›å»ºæ£€ç´¢å™¨
            retriever = self.index.as_retriever(similarity_top_k=top_k)
            
            # æ‰§è¡Œæ£€ç´¢
            nodes = retriever.retrieve(query)
            
            return nodes
            
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        if not self.client:
            return {"status": "not_initialized"}
        
        try:
            return {
                "status": "initialized",
                "vector_db_path": self.vector_db_path,
                "collection_name": self.collection_name,
                "documents_count": self.collection.count(),
                "embeddings_model": self.embeddings.__class__.__name__
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def clear_database(self) -> bool:
        """æ¸…ç©ºå‘é‡æ•°æ®åº“"""
        if not self.client:
            print("âŒ å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return False
        
        try:
            # åˆ é™¤é›†åˆ
            self.client.delete_collection(name=self.collection_name)
            
            # é‡æ–°åˆ›å»ºé›†åˆ
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            
            # é‡æ–°åˆå§‹åŒ–ç´¢å¼•
            self._init_vector_database()
            
            print("âœ… æˆåŠŸæ¸…ç©ºå‘é‡æ•°æ®åº“")
            return True
            
        except Exception as e:
            print(f"âŒ æ¸…ç©ºæ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def backup_database(self, backup_path: str) -> bool:
        """å¤‡ä»½å‘é‡æ•°æ®åº“"""
        if not self.client:
            print("âŒ å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return False
        
        try:
            import shutil
            shutil.copytree(self.vector_db_path, backup_path)
            print(f"âœ… æˆåŠŸå¤‡ä»½æ•°æ®åº“åˆ°: {backup_path}")
            return True
            
        except Exception as e:
            print(f"âŒ å¤‡ä»½æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def restore_database(self, backup_path: str) -> bool:
        """ä»å¤‡ä»½æ¢å¤å‘é‡æ•°æ®åº“"""
        try:
            import shutil
            # æ¸…ç©ºå½“å‰æ•°æ®åº“
            if os.path.exists(self.vector_db_path):
                shutil.rmtree(self.vector_db_path)
            
            # æ¢å¤å¤‡ä»½
            shutil.copytree(backup_path, self.vector_db_path)
            
            # é‡æ–°åˆå§‹åŒ–
            self._init_vector_database()
            
            print(f"âœ… æˆåŠŸä»å¤‡ä»½æ¢å¤æ•°æ®åº“: {backup_path}")
            return True
            
        except Exception as e:
            print(f"âŒ æ¢å¤æ•°æ®åº“å¤±è´¥: {e}")
            return False


class VectorDatabaseFactory:
    """å‘é‡æ•°æ®åº“å·¥å‚"""
    
    @staticmethod
    def create(embeddings=None,
               vector_db_path: str = None,
               collection_name: str = None) -> VectorDatabase:
        """åˆ›å»ºå‘é‡æ•°æ®åº“å®ä¾‹"""
        return VectorDatabase(
            embeddings=embeddings,
            vector_db_path=vector_db_path,
            collection_name=collection_name
        )


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import json
    from llama_index.core import Document
    from langchain_ollama.embeddings import OllamaEmbeddings

    # 1. è®¾ç½®å‚æ•°
    JSON_FILE_PATH = "/home/lx/drug-ragLLM/merged_20250923_195353.json"
    VECTOR_DB_PATH = "./chroma_store"
    COLLECTION_NAME = "drug_info"
    EMBEDDING_MODEL = "bge-m3"
    OLLAMA_BASE_URL = "http://localhost:11434"

    # 2. åˆ›å»ºå‘é‡æ•°æ®åº“å®ä¾‹
    print("åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_BASE_URL)
    
    print(f"åˆ›å»ºæˆ–è¿æ¥åˆ°å‘é‡æ•°æ®åº“ at {VECTOR_DB_PATH} with collection {COLLECTION_NAME}...")
    vector_db = VectorDatabaseFactory.create(
        embeddings=embeddings,
        vector_db_path=VECTOR_DB_PATH,
        collection_name=COLLECTION_NAME
    )

    # 3. æ¸…ç©ºæ•°æ®åº“ (å¯é€‰, ç¡®ä¿ä»ä¸€ä¸ªå¹²å‡€çš„çŠ¶æ€å¼€å§‹)
    print("æ¸…ç©ºæ•°æ®åº“...")
    vector_db.clear_database()

    # 4. åŠ è½½å¹¶å¤„ç†JSONæ•°æ®
    print(f"ä» {JSON_FILE_PATH} åŠ è½½æ•°æ®...")
    try:
        with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•ã€‚")
    except Exception as e:
        print(f"âŒ åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)

    # 5. å°†JSONæ•°æ®è½¬æ¢ä¸ºDocumentå¯¹è±¡
    print("æ­£åœ¨å°†æ•°æ®è½¬æ¢ä¸ºDocumentå¯¹è±¡...")
    documents = []
    # æ‰©å±•åçš„å­—æ®µä¸­æ–‡åç§°æ˜ å°„
    field_map = {
        "drug_name": "è¯ç‰©åç§°",
        "suitable_for": "é€‚ç”¨äººç¾¤",
        "contraindications": "ç¦å¿Œç—‡",
        "treats": "æ²»ç–—ç—…ç—‡",
        "symptoms": "ç›¸å…³ç—‡çŠ¶",
        "common_adverse_reactions": "å¸¸è§ä¸è‰¯ååº”",
        "serious_adverse_reactions": "ä¸¥é‡ä¸è‰¯ååº”",
        "side_effects": "å‰¯ä½œç”¨",
        "drug_interactions": "è¯ç‰©ç›¸äº’ä½œç”¨",
        "interactions": "ç›¸äº’ä½œç”¨",
        "precautions": "æ³¨æ„äº‹é¡¹",
        "pharmacological_effects": "è¯ç†ä½œç”¨",
        "dosage_and_administration": "ç”¨æ³•ç”¨é‡",
        "dosage": "å‰‚é‡",
        "special_populations": "ç‰¹æ®Šäººç¾¤",
        "storage": "å‚¨å­˜æ–¹æ³•"
    }

    for item in data:
        # æ„å»ºæè¿°æ€§æ®µè½
        text_parts = []
        # ä½¿ç”¨ field_map çš„é¡ºåºæ¥æ„å»ºæ–‡æœ¬ï¼Œä»¥è·å¾—æ›´ä¸€è‡´çš„è¾“å‡º
        for key, readable_name in field_map.items():
            value = item.get(key)
            if value:
                # å¦‚æœå€¼æ˜¯åˆ—è¡¨ï¼Œç”¨é€—å·è¿æ¥
                if isinstance(value, list):
                    value_str = "ã€".join(map(str, value))
                else:
                    value_str = str(value)
                
                if value_str:
                    text_parts.append(f"{readable_name}ä¸ºâ€œ{value_str}â€")

        text_content = "ï¼›".join(text_parts) + "ã€‚"
        
        # ä½¿ç”¨ drug_name ä½œä¸ºæ–‡æ¡£ID
        doc_id = item.get("drug_name")
        
        # åˆ›å»ºDocumentæ—¶ä¸å†ä¼ é€’metadata
        documents.append(Document(text=text_content, doc_id=doc_id))
    
    print(f"æˆåŠŸåˆ›å»º {len(documents)} ä¸ªDocumentå¯¹è±¡ã€‚")

    # 6. æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
    print("æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“...")
    vector_db.add_documents(documents)
    
    # 7. è·å–ç»Ÿè®¡ä¿¡æ¯å¹¶éªŒè¯
    stats = vector_db.get_stats()
    print(f"å‘é‡æ•°æ®åº“ç»Ÿè®¡: {stats}")
    
    # 8. æµ‹è¯•æœç´¢
    if stats.get("documents_count", 0) > 0:
        print("\n--- æµ‹è¯•æœç´¢ ---")
        query = "BTKæŠ‘åˆ¶å‰‚é€‚ç”¨äºå“ªäº›ç—…ç—‡?"
        print(f"æŸ¥è¯¢: {query}")
        results = vector_db.search(query, top_k=3)
        
        print(f"æœç´¢åˆ° {len(results)} ä¸ªç»“æœ:")
        for i, node in enumerate(results):
            print(f"\nç»“æœ {i+1} (å¾—åˆ†: {node.score:.3f}):")
            print(node.node.get_content()[:300] + "...")
    else:
        print("æ•°æ®åº“ä¸ºç©ºï¼Œè·³è¿‡æµ‹è¯•æœç´¢ã€‚")